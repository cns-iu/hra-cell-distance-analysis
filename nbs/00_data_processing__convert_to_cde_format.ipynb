{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing: Convert to CDE Format\n",
    "\n",
    "> Convert all datasets from original format to CDE format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import requests\n",
    "import scanpy as sc\n",
    "import shutil\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# suppress warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = \"/u/yashjain/hra-cell-distance-analysis/data\"\n",
    "orig_filedir = \"data-original\"\n",
    "dest_filedir = \"data-processed-nodes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load your data\n",
    "def load_data(path):\n",
    "    data = pd.read_csv(path)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "        print(f\"Directory '{directory}' created successfully.\")\n",
    "    else:\n",
    "        print(f\"Directory '{directory}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hubmap_uuid(hubmap_id):\n",
    "    # Construct the API URL\n",
    "    base_url = \"https://entity.api.hubmapconsortium.org/entities/\"\n",
    "    url = base_url + hubmap_id\n",
    "\n",
    "    try:\n",
    "        # Send GET request to the API\n",
    "        response = requests.get(url)\n",
    "        \n",
    "        # Check if the request was successful\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the JSON response\n",
    "        data = response.json()\n",
    "        \n",
    "        # Extract the UUID\n",
    "        uuid = data.get(\"uuid\")\n",
    "        \n",
    "        if uuid:\n",
    "            return uuid\n",
    "        else:\n",
    "            return \"UUID not found in the response\"\n",
    "    \n",
    "    except requests.RequestException as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14 ['bonemarrow-codex-chop', 'colon-cycif-sorgerlab', 'colon-xenium-stanford', 'esophagus-codex-stanford', 'intestine-codex-stanford', 'lung-codex-urmc', 'lymphnode-codex-yale', 'maternalfetalinterface-mibitof-stanford', 'oralcavity-codex-czi', 'pancreas-geomx-ufl', 'skin-celldive-ge', 'skin-confocal-sorgerlab', 'spleen-codex-ufl', 'tonsil-codex-stanford']\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(os.path.join(basepath, orig_filedir))), os.listdir(os.path.join(basepath, orig_filedir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'data-processed-nodes' already exists and has been removed. New directory will be created.\n"
     ]
    }
   ],
   "source": [
    "# Create destination directory. Overwrite if it exists.\n",
    "if os.path.exists(os.path.join(basepath, dest_filedir)):\n",
    "    shutil.rmtree(os.path.join(basepath, dest_filedir))\n",
    "    print(f\"Directory '{dest_filedir}' already exists and has been removed. New directory will be created.\")\n",
    "else:\n",
    "    print(f\"Directory '{dest_filedir}' does not exist and will be created.\")\n",
    "os.makedirs(os.path.join(basepath, dest_filedir), exist_ok=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processsing Individual Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intestine-codex-stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NK' 'Enterocyte' 'MUC1+ Enterocyte' 'TA' 'CD66+ Enterocyte' 'Paneth'\n",
      " 'Smooth muscle' 'M1 Macrophage' 'Goblet' 'Neuroendocrine'\n",
      " 'CD57+ Enterocyte' 'Lymphatic' 'CD8+ T' 'DC' 'M2 Macrophage' 'B'\n",
      " 'Neutrophil' 'Endothelial' 'Cycling TA' 'Plasma' 'CD4+ T cell' 'Stroma'\n",
      " 'Nerve' 'ICC' 'CD7+ Immune']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/intestine-codex-stanford' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"intestine-codex-stanford\"\n",
    "\n",
    "raw_filepath = os.path.join(basepath, orig_filedir, dataset_name, \"23_09_CODEX_HuBMAP_alldata_Dryad_merged.csv\")\n",
    "\n",
    "data_new = load_data(raw_filepath)\n",
    "\n",
    "# Filename to HuBMAP ID mapping.\n",
    "mapping_file = os.path.join(basepath, orig_filedir, dataset_name, \"filename-to-hubmap-mapping.csv\")\n",
    "mapping_df = load_data(mapping_file)\n",
    "\n",
    "# Convert the mapping dataframe to dictionary. Key is Filename and value is HuBMAP ID.\n",
    "filename_to_id = dict(zip(mapping_df[\"Filename\"], mapping_df[\"HuBMAP ID\"]))\n",
    "\n",
    "# Store types of unique regions before splitting\n",
    "# column is \"unique_region\"\n",
    "unique_regions = data_new[\"unique_region\"].unique()\n",
    "\n",
    "data_new.rename(columns={\"cell_type\": \"Cell Type\"}, inplace=True)\n",
    "\n",
    "# Store types of cell before splitting\n",
    "cell_types = data_new[\"Cell Type\"].unique()\n",
    "print(cell_types)\n",
    "\n",
    "# Take the column 'unique_region' to split from the actual column names of data frame\n",
    "column_to_split = \"unique_region\"\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for label in unique_regions:\n",
    "    # If label contains \"extra-nodes\" then skip.\n",
    "    if \"extra\" in label:\n",
    "        continue\n",
    "\n",
    "    # Create another sub data frame using the value for the value of the column each time\n",
    "    df_label = data_new[data_new[column_to_split] == label]\n",
    "\n",
    "    df_Region_1 = df_label[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "    # Calculate μm per px\n",
    "    micro_per_pixel = 0.37742\n",
    "    scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "    df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "    df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "    label_clean = label.replace(\"-\", \"_\")\n",
    "    label_clean = label_clean.replace(\" \", \"\")\n",
    "    # Write to the file using pandas to_csv\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label_clean}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "    # Generate dataset.json file for HRAPoP. Get the UUID for the HuBMAP ID from Entity API.\n",
    "    id = \"https://entity.api.hubmapconsortium.org/entities/\" + get_hubmap_uuid(filename_to_id[f\"{label_clean}-nodes\"])\n",
    "    dataset_json = {\n",
    "        \"@id\": id\n",
    "    }\n",
    "\n",
    "    with open(f\"{target_dir}/{label_clean}-dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## tonsil-codex-stanford\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Innate' 'PDPN' 'Endothelial' 'B' 'T' 'Squamous_epithelial' 'Stroma'\n",
      " 'SmoothMuscle' 'Plasma' 'Nerve' 'Glandular_epi' 'Secretory_epithelial'\n",
      " 'Paneth']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/tonsil-codex-stanford' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"tonsil-codex-stanford\"\n",
    "\n",
    "raw_filepath = os.path.join(basepath, orig_filedir, dataset_name, \"BE_Tonsil_l3_dryad.csv\")\n",
    "\n",
    "data_new = load_data(raw_filepath)\n",
    "\n",
    "# Store types of unique regions before splitting\n",
    "# column is \"unique_region\"\n",
    "unique_regions = data_new[\"sample_name\"].unique()\n",
    "\n",
    "data_new.rename(columns={\"cell_type\": \"Cell Type\"}, inplace=True)\n",
    "\n",
    "# Store types of cell before splitting\n",
    "cell_types = data_new[\"Cell Type\"].unique()\n",
    "print(cell_types)\n",
    "\n",
    "# Take the column 'unique_region' to split from the actual column names of data frame\n",
    "column_to_split = \"sample_name\"\n",
    "\n",
    "for label in unique_regions:\n",
    "    # Create another sub data frame using the value for the value of the column each time\n",
    "    if label == \"tonsil\":\n",
    "        df_label = data_new[data_new[column_to_split] == label]\n",
    "\n",
    "        df_Region_1 = df_label[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "        # Calculate μm per px\n",
    "        micro_per_pixel = 0.377  \n",
    "        scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "        df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "        df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "        target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "        create_directory(target_dir)\n",
    "        label_clean = label.replace(\"-\", \"_\")\n",
    "        label_clean = label_clean.replace(\" \", \"\")\n",
    "        # Write to the file using pandas to_csv\n",
    "        df_Region_1.to_csv(f\"{target_dir}/{label_clean}-nodes.csv\", index=False, header=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## esophagus-codex-stanford \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Innate' 'PDPN' 'Endothelial' 'B' 'T' 'Squamous_epithelial' 'Stroma'\n",
      " 'SmoothMuscle' 'Plasma' 'Nerve' 'Glandular_epi' 'Secretory_epithelial'\n",
      " 'Paneth']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/esophagus-codex-stanford' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"esophagus-codex-stanford\"\n",
    "\n",
    "raw_filepath = os.path.join(basepath, orig_filedir, dataset_name, \"BE_Tonsil_l3_dryad.csv\")\n",
    "\n",
    "data_new = load_data(raw_filepath)\n",
    "\n",
    "# Store types of unique regions before splitting\n",
    "# column is \"unique_region\"\n",
    "unique_regions = data_new[\"sample_name\"].unique()\n",
    "\n",
    "data_new.rename(columns={\"cell_type\": \"Cell Type\"}, inplace=True)\n",
    "\n",
    "# Store types of cell before splitting\n",
    "cell_types = data_new[\"Cell Type\"].unique()\n",
    "print(cell_types)\n",
    "\n",
    "# Take the column 'unique_region' to split from the actual column names of data frame\n",
    "column_to_split = \"sample_name\"\n",
    "\n",
    "for label in unique_regions:\n",
    "    # Create another sub data frame using the value for the value of the column each time\n",
    "    if label == \"Barretts Esophagus\":\n",
    "        df_label = data_new[data_new[column_to_split] == label]\n",
    "\n",
    "        df_Region_1 = df_label[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "        # Calculate μm per px\n",
    "        micro_per_pixel = 0.377  \n",
    "        scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "        df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "        df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "        target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "        create_directory(target_dir)\n",
    "        \n",
    "        # Write to the file using pandas to_csv\n",
    "        df_Region_1.to_csv(f\"{target_dir}/esophagus-nodes.csv\", index=False, header=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colon-cycif-sorgerlab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Other' 'PDL1+ Macrophage' 'Tumor/Epithelial' 'Lymphocyte(III)' 'Treg'\n",
      " 'Endothelial' 'Muscle/Fibroblast' 'Tc cell' 'PDL1+ Tumor/Epithelial'\n",
      " 'Macrophage(IV)' 'B cells' 'PD1+ Tc' 'Macrophage(III)' 'DN Lymphocyte'\n",
      " 'DP Lymphocyte' 'T helper' 'Macrophage(II)' 'Macrophage(I)'\n",
      " 'Ki67+ Tumor/Epithelial' 'PD1+ T helper' 'PDL1+ lymphocyte']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/colon-cycif-sorgerlab' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"colon-cycif-sorgerlab\"\n",
    "\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name)\n",
    "\n",
    "# Preprocess to merge CT labels.\n",
    "# List of filenames\n",
    "filenames = [\n",
    "    \"Reg_Celltype_CRC01002.csv\", \"Reg_Celltype_CRC01007.csv\", \"Reg_Celltype_CRC01014.csv\", \n",
    "    \"Reg_Celltype_CRC01020.csv\", \"Reg_Celltype_CRC01025.csv\", \"Reg_Celltype_CRC01029.csv\", \n",
    "    \"Reg_Celltype_CRC01034.csv\", \"Reg_Celltype_CRC01039.csv\", \"Reg_Celltype_CRC01044.csv\", \n",
    "    \"Reg_Celltype_CRC01049.csv\", \"Reg_Celltype_CRC01050.csv\", \"Reg_Celltype_CRC01051.csv\", \n",
    "    \"Reg_Celltype_CRC01052.csv\", \"Reg_Celltype_CRC01054.csv\", \"Reg_Celltype_CRC01059.csv\", \n",
    "    \"Reg_Celltype_CRC01064.csv\", \"Reg_Celltype_CRC01069.csv\", \"Reg_Celltype_CRC01074.csv\", \n",
    "    \"Reg_Celltype_CRC01078.csv\", \"Reg_Celltype_CRC01084.csv\", \"Reg_Celltype_CRC01086.csv\", \n",
    "    \"Reg_Celltype_CRC01091.csv\", \"Reg_Celltype_CRC01097.csv\", \"Reg_Celltype_CRC01102.csv\", \n",
    "    \"Reg_Celltype_CRC01106.csv\"\n",
    "]\n",
    "\n",
    "# Read CSV files, add filename column, and combine\n",
    "df_list = []\n",
    "for filename in filenames:\n",
    "    df = pd.read_csv(os.path.join(raw_filedir, filename))\n",
    "    # Extract CRCXXXXX part from the filename\n",
    "    crc_code = filename.split('.')[0].split('_')[2]\n",
    "    df['Layer'] = crc_code\n",
    "    df_list.append(df)\n",
    "\n",
    "combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Read Celltype_reference_table.csv\n",
    "ref_df = pd.read_csv(os.path.join(raw_filedir, \"Celltype_reference_table.csv\"))\n",
    "\n",
    "# Create a dictionary for mapping NewType to Name and Category\n",
    "ref_dict = ref_df.set_index('NewType')[['Name', 'Category']].to_dict('index')\n",
    "\n",
    "# Add new columns \"Cell Type\" and \"Category\" to the combined csv file\n",
    "combined_df['Cell Type'] = combined_df['NewType'].map(lambda x: ref_dict.get(x, {}).get('Name', ''))\n",
    "combined_df['Category'] = combined_df['NewType'].map(lambda x: ref_dict.get(x, {}).get('Category', ''))\n",
    "\n",
    "combined_df.rename(columns={\"Xr\": \"x\", \"Yr\": \"y\"}, inplace=True)\n",
    "\n",
    "data_new = combined_df \n",
    "\n",
    "unique_regions = data_new[\"Layer\"].unique()\n",
    "\n",
    "cell_types = data_new[\"Cell Type\"].unique()\n",
    "print(cell_types)\n",
    "\n",
    "column_to_split = \"Layer\"\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for label in unique_regions:\n",
    "    # Create another sub data frame using the value for the value of the column each time\n",
    "    df_label = data_new[data_new[column_to_split] == label]\n",
    "\n",
    "    df_Region_1 = df_label[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "    # Calculate μm per px\n",
    "    micro_per_pixel = 0.65\n",
    "    scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "    df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "    df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "    # Write to the file using pandas to_csv\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "    # Generate dataset.json file for HRAPoP. Paper DOI: https://doi.org/10.1016/j.cell.2022.12.028\n",
    "    id = f\"https://doi.org/10.1016/j.cell.2022.12.028#{label}\"\n",
    "    dataset_json = {\n",
    "        \"@id\": id\n",
    "    }\n",
    "\n",
    "    with open(f\"{target_dir}/{label}-dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset_json, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## colon-xenium-stanford "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Immature Goblet' 'Tuft' 'TA1' 'CD4+' 'Pericytes' 'Macrophages'\n",
      " 'CyclingTA' 'Cancer Associated Fibroblasts' 'Best4+ Enterocytes' 'Stem'\n",
      " 'CD8+' 'Endothelial' 'TA2' 'Myofibroblasts/Smooth Muscle 3'\n",
      " 'Unknown_lowcount' 'Myofibroblasts/Smooth Muscle 1' 'Crypt Fibroblasts 3'\n",
      " 'Crypt Fibroblasts 1' 'Glia' 'Crypt Fibroblasts 4' 'Adipocytes' 'Plasma'\n",
      " 'GC' 'Mast' 'Enteroendocrine' 'Tregs' 'Lymphatic endothelial cells'\n",
      " 'Goblet' 'Immature Enterocytes' 'Enterocyte Progenitors' 'Enterocytes'\n",
      " 'Myofibroblasts/Smooth Muscle 2' 'Naive T' 'Villus Fibroblasts WNT5B+'\n",
      " 'Naive B' 'Neurons' 'Memory B' 'Crypt Fibroblasts 2' 'ILCs' 'Unknown'\n",
      " 'DC']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/colon-xenium-stanford' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"colon-xenium-stanford\"\n",
    "\n",
    "raw_filepath = os.path.join(basepath, orig_filedir, dataset_name, \"xenium_polyp_29sections.txt\")\n",
    "\n",
    "data_new = pd.read_csv(raw_filepath, sep='\\t')\n",
    "\n",
    "# Store types of unique regions before splitting\n",
    "# column is \"unique_region\"\n",
    "unique_regions = data_new[\"layer\"].unique()\n",
    "\n",
    "data_new.rename(columns={\"cell_type\": \"Cell Type\", \"x_align\": \"x\", \"y_align\": \"y\"}, inplace=True)\n",
    "\n",
    "# Store types of cell before splitting\n",
    "cell_types = data_new[\"Cell Type\"].unique()\n",
    "print(cell_types)\n",
    "\n",
    "# Take the column 'unique_region' to split from the actual column names of data frame\n",
    "column_to_split = \"layer\"\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for label in unique_regions:\n",
    "    # Create another sub data frame using the value for the value of the column each time\n",
    "    df_label = data_new[data_new[column_to_split] == label]\n",
    "\n",
    "    df_Region_1 = df_label[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "    # Write to the file using pandas to_csv\n",
    "    df_Region_1.to_csv(f\"{target_dir}/layer_{label}-nodes.csv\", index=False, header=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## lymphnode-codex-yale \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/lymphnode-codex-yale' created successfully.\n",
      "{'DC_CCR7+', 'B_plasma', 'Mast', 'T_CD8+_cytotoxic', 'Macrophages_M2', 'T_CD8+_naive', 'DC_pDC', 'VSMC', 'T_CD4+_TfH_GC', 'B_GC_DZ', 'B_naive', 'B_GC_LZ', 'T_CD4+', 'DC_cDC1', 'NK', 'B_mem', 'T_CD4+_naive', 'B_preGC', 'B_IFN', 'B_activated', 'T_CD4+_TfH', 'B_GC_prePB', 'Monocytes', 'NKT', 'Endothelial', 'T_CD8+_CD161+', 'T_TfR', 'T_TIM3+', 'DC_cDC2', 'FDC', 'T_Treg', 'Macrophages_M1', 'B_Cycling', 'ILC'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"lymphnode-codex-yale\"\n",
    "\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name)\n",
    "\n",
    "ct = set()\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for raw_filename in os.listdir(raw_filedir):\n",
    "    data_new = load_data(os.path.join(raw_filedir, raw_filename))\n",
    "\n",
    "    data_new.rename(columns={\"celltype\": \"Cell Type\"}, inplace=True)\n",
    "\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].apply(lambda x: x.replace('Endo', 'Endothelial'))\n",
    "\n",
    "    cell_types = data_new[\"Cell Type\"].unique()\n",
    "    \n",
    "    for i in cell_types:\n",
    "        ct.add(i)\n",
    "\n",
    "    df_Region_1 = data_new[[\"x\", \"y\", \"Cell Type\"]]\n",
    "    \n",
    "    # Write to the file using pandas to_csv\n",
    "    label = raw_filename.split(\".\")[0].split(\"_\")[0]\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## maternalfetalinterface-mibitof-stanford \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Mac2a' 'other' 'NK1' 'Fibroblasts' 'NKT' 'Endothelial' 'Myofibroblasts'\n",
      " 'Mac1a' 'EVT1a' 'Mac1b' 'CD8T' 'EVT1b' 'Mac2c' 'NK2' 'muscle' 'NK3'\n",
      " 'EVT2' 'Mac2b' 'DC' 'Glandular' 'CD4T' 'EVT1c' 'NK4' 'Mast' 'Treg'\n",
      " 'Placental_Mac']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/maternalfetalinterface-mibitof-stanford' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"maternalfetalinterface-mibitof-stanford\"\n",
    "\n",
    "raw_filepath = os.path.join(basepath, orig_filedir, dataset_name, \"Supplementary_table_3_single_cells_updated.csv\")\n",
    "\n",
    "data = load_data(raw_filepath)\n",
    "\n",
    "data_new = data[data['overlap_decidua'] == 1.0]\n",
    "\n",
    "# Store types of unique regions before splitting\n",
    "# column is \"unique_region\"\n",
    "unique_regions = data_new[\"Point\"].unique()\n",
    "\n",
    "data_new.rename(columns={'centroid0': 'x', 'centroid1': 'y', 'lineage': 'Cell Type' }, inplace=True)\n",
    "\n",
    "# Store types of cell before splitting\n",
    "cell_types = data_new[\"Cell Type\"].unique()\n",
    "print(cell_types)\n",
    "\n",
    "# Take the column 'unique_region' to split from the actual column names of data frame\n",
    "column_to_split = \"Point\"\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for label in unique_regions:\n",
    "    # Create another sub data frame using the value for the value of the column each time\n",
    "    df_label = data_new[data_new[column_to_split] == label]\n",
    "\n",
    "    df_Region_1 = df_label[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "    # Calculate μm per px\n",
    "    micro_per_pixel = 0.391\n",
    "    scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "    df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "    df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "    # Write to the file using pandas to_csv\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## oralcavity-codex-czi \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/oralcavity-codex-czi' already exists.\n",
      "{'Others', 'Melanocytes', 'myofibroblast', 'Plasma Cells', 'Skeletal Myocytes', 'Suprabasal Keratinocytes', 'Dendritic Cells', 'Macrophage', 'Vascular Endothelial Cells', 'CD8 T Cells', 'Monocyte-Macrophage', 'Glial/Neuron', 'Basal Keratincytes', 'Ducts', 'B Cells', 'Epithelial', 'Tc', 'Lymphatic Endothelial Cells', 'Adipocytes', 'Myoepithelial Cells', 'Neutrophils', 'Acinar Cells', 'Lymphatic Vascular Cells', 'DP', 'Mural Cells', 'CD4 T Cells', 'gd T Cells', 'VEC Progen', 'Ionocytes', 'Langerhans Cells', 'Fibroblasts', 'Th', 'Acini', 'Mast Cells', 'Merkel Cells', 'NK Cells', 'Ductal Epithelial Cells', 'Keratinocyte', 'Treg'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"oralcavity-codex-czi\"\n",
    "\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name)\n",
    "\n",
    "ct = set()\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for raw_filename in os.listdir(raw_filedir):\n",
    "    data_new = load_data(os.path.join(raw_filedir, raw_filename))\n",
    "\n",
    "    data_new.rename(columns={\"TACIT\": \"Cell Type\"}, inplace=True)\n",
    "    data_new.rename(columns={\"X\": \"x\"}, inplace=True)\n",
    "    data_new.rename(columns={\"Y\": \"y\"}, inplace=True)\n",
    "\n",
    "    # If \"Cell Type\" contains \"VECs\", replace with \"Endothelial (Vascular)\". Else if, \"Cell Type\" contains \"VEC\", replace with \"Endothelial (Vascular)\"\n",
    "    # Else if \"Cell Type\" contains \"Endothelial Cells\", replace with \"Endothelial (Vascular)\". Else, keep the same.\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('VECs', 'Vascular Endothelial Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('VEC', 'Vascular Endothelial Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Endothelial Cells', 'Vascular Endothelial Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('LECs', 'Lymphatic Endothelial Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Melanocyte', 'Melanocytes')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('B cells', 'B Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('B Cell', 'B Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('NK cells', 'NK Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Merkel cells', 'Merkel Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Neutrophil', 'Neutrophils')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Fibroblast', 'Fibroblasts')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('DC cells', 'Dendritic Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Myoepithelial', 'Myoepithelial Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Ductal Epithelial  Cells', 'Ductal Epithelial Cells')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('myfibroblast', 'myofibroblast')\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].replace('Myfibroblast', 'myofibroblast')\n",
    "\n",
    "    cell_types = data_new[\"Cell Type\"].unique()\n",
    "\n",
    "    for i in cell_types:\n",
    "        ct.add(i)\n",
    "\n",
    "    df_Region_1 = data_new[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "    # Calculate μm per px\n",
    "    micro_per_pixel = 0.5   # This is the pixel size but no need to scale since data is already given in micrometers.\n",
    "    scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "    df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "    df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "    # Write to the file using pandas to_csv\n",
    "    label = raw_filename.split(\".\")[0]\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## pancreas-geomx-ufl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/pancreas-geomx-ufl' created successfully.\n",
      "{'Beta cell', 'Ductal cell', 'unknown', 'Endothelial'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"pancreas-geomx-ufl\"\n",
    "\n",
    "# Loop through all csv files in the directory \"vccf-data-original/unpublished/pancreas-geomx-ufl/original-unprocessed/\".\n",
    "# For each file, read it into a pandas dataframe and print the first few rows.\n",
    "for file in os.listdir(os.path.join(basepath, orig_filedir, dataset_name, \"original-unprocessed\")):\n",
    "    if file.endswith(\".csv\"):\n",
    "        # read csv. Coordinates are in micrometers.\n",
    "        data = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"original-unprocessed\", file))\n",
    "\n",
    "        # Rename column CD3134 with CD31/34\n",
    "        data = data.rename(columns={\"CD3134\": \"CD31/34\"})\n",
    "        # Replace \"Negative\" with \"-\" and \"Positive\" with \"+\" in column \"Insulin\"\n",
    "        data[\"Insulin\"] = data[\"Insulin\"].replace(\"Negative\", \"Insulin-\")\n",
    "        data[\"Insulin\"] = data[\"Insulin\"].replace(\"Positive\", \"Insulin+\")\n",
    "\n",
    "        # Replace \"Negative\" with \"-\" and \"Positive\" with \"+\" in column \"PanCK\"\n",
    "        data[\"PanCK\"] = data[\"PanCK\"].replace(\"Negative\", \"PanCK-\")\n",
    "        data[\"PanCK\"] = data[\"PanCK\"].replace(\"Positive\", \"PanCK+\")\n",
    "\n",
    "        # Replace \"Negative\" with \"-\" and \"Positive\" with \"+\" in column \"CD31/34\"\n",
    "        data[\"CD31/34\"] = data[\"CD31/34\"].replace(\"Negative\", \"CD31/34-\")\n",
    "        data[\"CD31/34\"] = data[\"CD31/34\"].replace(\"Positive\", \"CD31/34+\")\n",
    "\n",
    "        # Merge columns \"Insulin\", \"PanCK\" and \"CD31/34\" into a new column \"CellType\"\n",
    "        data[\"CellType\"] = data[\"Insulin\"] + \" \" + data[\"PanCK\"] + \" \" + data[\"CD31/34\"]\n",
    "\n",
    "        # drop columns \"Insulin\", \"PanCK\" and \"CD31/34\"\n",
    "        data = data.drop(columns=[\"Insulin\", \"PanCK\", \"CD31/34\"])\n",
    "\n",
    "        # Replace markers with cell types.\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin- PanCK- CD31/34+\", \"Endothelial\")\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin- PanCK+ CD31/34-\", \"Ductal cell\")\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin+ PanCK- CD31/34-\", \"Beta cell\")\n",
    "\n",
    "        # Replace all other marker combinations with \"unknown\"\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin+ PanCK+ CD31/34-\", \"unknown\")\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin- PanCK+ CD31/34+\", \"unknown\")\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin+ PanCK- CD31/34+\", \"unknown\")\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin- PanCK- CD31/34-\", \"unknown\")\n",
    "        data[\"CellType\"] = data[\"CellType\"].replace(\"Insulin+ PanCK+ CD31/34+\", \"unknown\")\n",
    "\n",
    "        # Rename Cell X Coordinate to x, Cell Y Coordinate to y, and CellType to Cell Type.\n",
    "        data = data.rename(columns={\"xcoord\": \"x\", \"ycoord\": \"y\", \"CellType\": \"Cell Type\"})\n",
    "\n",
    "        # Save the new dataframe to a new csv file.\n",
    "        data.to_csv(os.path.join(basepath, orig_filedir, dataset_name, \"cell-type-annotated\", file), index=False)\n",
    "\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name, \"cell-type-annotated\")\n",
    "\n",
    "ct = set()\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for raw_filename in os.listdir(raw_filedir):\n",
    "    data_new = load_data(os.path.join(raw_filedir, raw_filename))\n",
    "    cell_types = data_new[\"Cell Type\"].unique()\n",
    "    for i in cell_types:\n",
    "        ct.add(i)\n",
    "    df_Region_1 = data_new[[\"x\", \"y\", \"Cell Type\"]]\n",
    "    # Write to the file using pandas to_csv\n",
    "    label = raw_filename.split(\".\")[0].split(\"_\")[0]\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## skin-celldive-ge \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/skin-celldive-ge' created successfully.\n",
      "{'KI67', 'Endothelial', 'P53', 'DDB2', 'CD68', 'T-Reg', 'T-Killer', 'T-Helper'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"skin-celldive-ge\"\n",
    "\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name, \"regions\")\n",
    "\n",
    "# Filename to HuBMAP ID dictionary.\n",
    "# This is used to map the filename to the HuBMAP ID. Source: https://www.biorxiv.org/content/10.1101/2023.10.05.560733v2 \n",
    "filename_to_id = {\n",
    "    \"region_1\": \"HBM732.FZVZ.656\",\n",
    "    \"region_2\": \"HBM747.SPWK.779\",\n",
    "    \"region_3\": \"HBM398.NCVN.256\",\n",
    "    \"region_4\": \"HBM746.VTDZ.959\",\n",
    "    \"region_5\": \"HBM875.SBHJ.939\",\n",
    "    \"region_6\": \"HBM867.NMXL.794\", # Excluded in original analysis\n",
    "    \"region_7\": \"HBM666.JCGS.862\",\n",
    "    \"region_8\": \"HBM592.JGSQ.253\",\n",
    "    \"region_9\": \"HBM494.XDQW.356\",\n",
    "    \"region_10\": \"HBM238.ZKPC.934\",\n",
    "    \"region_11\": \"HBM975.FVCG.922\",\n",
    "    \"region_12\": \"HBM674.XQFQ.364\", # Excluded in original analysis\n",
    "}\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "ct = set()\n",
    "for region in os.listdir(raw_filedir):\n",
    "    # Exclude regions 6 and 12 as they were excluded in the original analysis.\n",
    "    if region == \"region_6\" or region == \"region_12\":\n",
    "        continue\n",
    "\n",
    "    raw_filename = os.path.join(raw_filedir, region, \"centroids.csv\")\n",
    "\n",
    "    data_new = pd.read_csv(raw_filename)\n",
    "\n",
    "    data_new.rename(columns={\"cell_type\": \"Cell Type\"}, inplace=True)\n",
    "    data_new.rename(columns={\"X\": \"x\"}, inplace=True)\n",
    "    data_new.rename(columns={\"Y\": \"y\"}, inplace=True)\n",
    "    data_new.rename(columns={\"Z\": \"z\"}, inplace=True)\n",
    "\n",
    "    # For consistency. Note that all endothelial cells in this dataset are blood endothelial\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].apply(lambda x: x.replace('CD31', 'Endothelial'))\n",
    "\n",
    "    # Drop \"skin\" coordinates.\n",
    "    data_new = data_new[data_new['Cell Type'] != 'Skin']\n",
    "\n",
    "    cell_types = data_new[\"Cell Type\"].unique()\n",
    "    for i in cell_types:\n",
    "        ct.add(i)\n",
    "    df_Region_1 = data_new[[\"x\", \"y\", \"z\", \"Cell Type\"]]\n",
    "\n",
    "    # Calculate μm per px\n",
    "    micro_per_pixel = 1  \n",
    "    scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "    df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "    df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "    df_Region_1[\"z\"] = scale * df_Region_1[\"z\"]\n",
    "\n",
    "    # Write to the file using pandas to_csv\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{region}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "    # Generate dataset.json file for HRAPoP. Get the UUID for the HuBMAP ID from Entity API.\n",
    "    id = \"https://entity.api.hubmapconsortium.org/entities/\" + get_hubmap_uuid(filename_to_id[region])\n",
    "    dataset_json = {\n",
    "        \"@id\": id\n",
    "    }\n",
    "\n",
    "    with open(f\"{target_dir}/{region}-dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset_json, f, indent=4)\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## skin-confocal-sorgerlab \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/skin-confocal-sorgerlab' created successfully.\n",
      "{'Unknown', 'Dendritic cells', 'Tumor', 'Endothelial', 'Macrophage', 'Other Immune', 'B cells', 'Myeloid', 'Tissue T', 'Langerhan cells', 'CD11B+ CD11C- cells', 'CD8 T', 'CD4 T', 'keratinocytes', 'T reg'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"skin-confocal-sorgerlab\"\n",
    "\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name)\n",
    "\n",
    "ct = set()\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for raw_filename in os.listdir(raw_filedir):\n",
    "\n",
    "    data_new = load_data(os.path.join(raw_filedir, raw_filename))\n",
    "\n",
    "    data_new.rename(columns={\"phenotype\": \"Cell Type\", \"X_centroid\": \"x\", \"Y_centroid\": \"y\", \"Z_centroid\": \"z\"}, inplace=True)\n",
    "\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].apply(lambda x: x.replace('endothelial', 'Endothelial'))\n",
    "\n",
    "    cell_types = data_new[\"Cell Type\"].unique()\n",
    "\n",
    "    for i in cell_types:\n",
    "        ct.add(i)\n",
    "\n",
    "    df_Region_1 = data_new[[\"x\", \"y\", \"z\", \"Cell Type\"]]\n",
    "    \n",
    "    # Write to the file using pandas to_csv\n",
    "    label = raw_filename.split(\".\")[0]\n",
    "\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "    # Generate dataset.json file for HRAPoP. Paper DOI: https://doi.org/10.1101/2023.11.10.566670\n",
    "    id = f\"https://doi.org/10.1101/2023.11.10.566670#{label}\"\n",
    "    dataset_json = {\n",
    "        \"@id\": id\n",
    "    }\n",
    "\n",
    "    with open(f\"{target_dir}/{label}-dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset_json, f, indent=4)\n",
    "\n",
    "print(ct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## spleen-codex-ufl \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/spleen-codex-ufl' created successfully.\n",
      "{'Myeloid cells', 'Sinusoidal cells', 'CD8 Memory T cells', 'Macrophages', 'indistinct', 'B cells, red pulp', 'Ki67 proliferating', 'Fol B cells', 'blood endothelial', 'Neutrophils/Monocytes', 'Podoplanin', 'CD4 Memory T cells'}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"spleen-codex-ufl\"\n",
    "ct = set()\n",
    "raw_filedir = os.path.join(basepath, orig_filedir, dataset_name)\n",
    "\n",
    "# Filename to HuBMAP ID dictionary.\n",
    "# This is used to map the filename to the HuBMAP ID. Source: https://www.biorxiv.org/content/10.1101/2023.10.05.560733v2 \n",
    "filename_to_id = {\n",
    "    \"FSLD\": \"HBM342.FSLD.938\",\n",
    "    \"KSFB\": \"HBM556.KSFB.592\",\n",
    "    \"NGPL\": \"HBM568.NGPL.345\",\n",
    "    \"PBVN\": \"HBM825.PBVN.284\",\n",
    "    \"PKHL\": \"HBM389.PKHL.936\",\n",
    "    \"XXCD\": \"HBM772.XXCD.697\",\n",
    "}\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for raw_filename in os.listdir(raw_filedir):\n",
    "\n",
    "    data_new = pd.read_csv(os.path.join(raw_filedir, raw_filename), sep=';')\n",
    "\n",
    "    data_new.rename(columns={\"celltypes_folBcombined\": \"Cell Type\"}, inplace=True)\n",
    "\n",
    "    # For consistency. Note that all endothelial cells in this dataset are blood endothelial\n",
    "    data_new[\"Cell Type\"] = data_new[\"Cell Type\"].apply(lambda x: x.replace('Blood endothelial', 'blood endothelial'))\n",
    "\n",
    "    cell_types = data_new[\"Cell Type\"].unique()\n",
    "    \n",
    "    # Add all elements in cell_types list to teh ct set\n",
    "    for i in cell_types:\n",
    "        ct.add(i)\n",
    "\n",
    "    df_Region_1 = data_new[[\"x\", \"y\", \"Cell Type\"]]\n",
    "\n",
    "    # Calculate μm per px\n",
    "    micro_per_pixel = 0.377  \n",
    "    scale = micro_per_pixel  # to convert given pixel in micro meter unit\n",
    "    df_Region_1[\"x\"] = scale * df_Region_1[\"x\"]\n",
    "    df_Region_1[\"y\"] = scale * df_Region_1[\"y\"]\n",
    "\n",
    "    # Find the most negative value in each column\n",
    "    shift_x = abs(df_Region_1['x'].min())\n",
    "    shift_y = abs(df_Region_1['y'].min())\n",
    "\n",
    "    # Shift all values in the columns to be positive\n",
    "    df_Region_1['x'] = df_Region_1['x'] + shift_x\n",
    "    df_Region_1['y'] = df_Region_1['y'] + shift_y\n",
    "    \n",
    "    # Write to the file using pandas to_csv\n",
    "    label = raw_filename.split('.')[0]\n",
    "    df_Region_1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "    # Generate dataset.json file for HRAPoP. Get the UUID for the HuBMAP ID from Entity API.\n",
    "    id = \"https://entity.api.hubmapconsortium.org/entities/\" + get_hubmap_uuid(filename_to_id[label])\n",
    "    dataset_json = {\n",
    "        \"@id\": id\n",
    "    }\n",
    "\n",
    "    with open(f\"{target_dir}/{label}-dataset.json\", \"w\") as f:\n",
    "        json.dump(dataset_json, f, indent=4)\n",
    "    \n",
    "print(ct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## lung-codex-urmc \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file: /u/yashjain/hra-cell-distance-analysis/data/data-original/lung-codex-urmc/D265_final_annotated.h5ad\n",
      "\n",
      "Data successfully saved to: /u/yashjain/hra-cell-distance-analysis/data/data-original/lung-codex-urmc/D265_final_annotated_cells.csv\n",
      "Reading file: /u/yashjain/hra-cell-distance-analysis/data/data-original/lung-codex-urmc/D115_final_annotated.h5ad\n",
      "\n",
      "Data successfully saved to: /u/yashjain/hra-cell-distance-analysis/data/data-original/lung-codex-urmc/D115_final_annotated_cells.csv\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/lung-codex-urmc' created successfully.\n",
      "['CD4_+_Tcell/macrophage' 'CD8+_T_cell_1' 'macrophage_3' 'ENDO_1'\n",
      " 'ENDO_CD8+_T_Cell' 'CD4+_T_cell_1' 'Lung_Epithelial_1' 'CAP_ENDO'\n",
      " 'macrophage_CD1c+_myeloidDC' 'ENDO_SMC' 'Endo_p'\n",
      " 'Lung_Epithelil_2_CD4+_T_cell' 'AT2_2' 'AT2_1' 'CD8+_T_cell_2'\n",
      " 'macrophage_2' 'macrophage_p' 'CD4+_T_cell_2' 'AT2_p' 'SMC_2' 'SMC_1'\n",
      " 'CD8+_T_cell_3' 'B_cell_1' 'Lung_Epithelial_p' 'CD8+_T_cell_CD_4+_T_cell'\n",
      " 'Lung_Epithelial_4' 'UNK_5_ambiguous' 'UNK_1_APC' 'CD4+_T_cell_3'\n",
      " 'B_cell_macrophage_p?' 'UNK_3_(col1a1-driven_cluster)'\n",
      " 'Lymphatic_Endothelium' 'UNK_4_(col1a1_driven_cluster)']\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"lung-codex-urmc\"\n",
    "\n",
    "# Process D265 Dataset.\n",
    "# Read the h5ad file\n",
    "# Specify your input and output file paths\n",
    "h5ad_path = os.path.join(basepath, orig_filedir, dataset_name, \"D265_final_annotated.h5ad\")\n",
    "output_csv_path = os.path.join(basepath, orig_filedir, dataset_name, \"D265_final_annotated_cells.csv\")\n",
    "\n",
    "print(f\"Reading file: {h5ad_path}\")\n",
    "adata = sc.read_h5ad(h5ad_path)\n",
    "\n",
    "# Initialize dictionary to store the data\n",
    "data_dict = {}\n",
    "\n",
    "# Extract spatial coordinates and split into x, y\n",
    "if 'spatial' in adata.obsm_keys():\n",
    "    spatial_coords = adata.obsm['spatial']\n",
    "    data_dict['x'] = spatial_coords[:, 0]\n",
    "    data_dict['y'] = spatial_coords[:, 1]\n",
    "else:\n",
    "    raise KeyError(\"'spatial' coordinates not found in obsm\")\n",
    "\n",
    "# Extract cell type calls from obs\n",
    "if 'cell_type_calls' in adata.obs:\n",
    "    data_dict['Cell Type'] = adata.obs['cell_type_calls'].tolist()\n",
    "else:\n",
    "    raise KeyError(\"'cell_type_calls' column not found in obs\")\n",
    "\n",
    "# Extract ontology mappings from uns\n",
    "if 'ontology_mappings' in adata.uns:\n",
    "    # Create a mapping dictionary from uns data\n",
    "    data_dict['Cell Ontology ID'] = adata.uns['ontology_mappings'][\"D265_ont_1\"].tolist()\n",
    "    data_dict['Cell Ontology ID_2'] = adata.uns['ontology_mappings'][\"D265_ont_2\"].tolist()\n",
    " \n",
    "else:\n",
    "    raise KeyError(\"'ontology_mappings' not found in uns\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "# In D265 dataset, remove space in Cell Type column where value is ENDO_1.\n",
    "df['Cell Type'] = df['Cell Type'].replace(' ENDO_1', 'ENDO_1')\n",
    "df['Cell Type'] = df['Cell Type'].replace(' macrophage', 'macrophage')\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nData successfully saved to: {output_csv_path}\")\n",
    "\n",
    "del adata\n",
    "del df\n",
    "del data_dict\n",
    "\n",
    "# Process D115 Dataset.\n",
    "# Read the h5ad file\n",
    "# Specify your input and output file paths\n",
    "h5ad_path = os.path.join(basepath, orig_filedir, dataset_name, \"D115_final_annotated.h5ad\")\n",
    "output_csv_path = os.path.join(basepath, orig_filedir, dataset_name, \"D115_final_annotated_cells.csv\")\n",
    "\n",
    "print(f\"Reading file: {h5ad_path}\")\n",
    "adata = sc.read_h5ad(h5ad_path)\n",
    "\n",
    "# Initialize dictionary to store the data\n",
    "data_dict = {}\n",
    "\n",
    "# Extract spatial coordinates and split into x, y\n",
    "if 'spatial' in adata.obsm_keys():\n",
    "    spatial_coords = adata.obsm['spatial']\n",
    "    data_dict['x'] = spatial_coords[:, 0]\n",
    "    data_dict['y'] = spatial_coords[:, 1]\n",
    "else:\n",
    "    raise KeyError(\"'spatial' coordinates not found in obsm\")\n",
    "\n",
    "# Extract cell type calls from obs\n",
    "if 'cell_type_calls' in adata.obs:\n",
    "    data_dict['Cell Type'] = adata.obs['cell_type_calls'].tolist()\n",
    "else:\n",
    "    raise KeyError(\"'cell_type_calls' column not found in obs\")\n",
    "\n",
    "# Extract ontology mappings from uns\n",
    "if 'ontology_mappings' in adata.uns:\n",
    "    # Create a mapping dictionary from uns data\n",
    "    # Note: Adjust this based on the actual structure of your ontology_mappings\n",
    "    data_dict['Cell Ontology ID'] = adata.uns['ontology_mappings'][\"D115_ont_1\"].tolist()\n",
    "    data_dict['Cell Ontology ID_2'] = adata.uns['ontology_mappings'][\"D115_ont_2\"].tolist()\n",
    "\n",
    "else:\n",
    "    raise KeyError(\"'ontology_mappings' not found in uns\")\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data_dict)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(output_csv_path, index=False)\n",
    "print(f\"\\nData successfully saved to: {output_csv_path}\")\n",
    "\n",
    "del adata\n",
    "del df\n",
    "del data_dict\n",
    "\n",
    "# Replace cell type names with cell labels from crosswalk file and generate cell-nodes files for both datasets. \n",
    "# Read D265 csv file.\n",
    "df1 = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"D265_final_annotated_cells.csv\"))\n",
    "\n",
    "# Drop duplicate rows in D265. Check if any duplicate rows.\n",
    "df1 = df1.drop_duplicates(subset=['x', 'y', 'Cell Type'])\n",
    "\n",
    "# # Read crosswalk file.\n",
    "# crosswalk = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"extras\", \"combined_unique_ct_list_with_cl_labels.csv\"))\n",
    "# # Keep unique rows in crosswalk file. Check uniqueness based on Cell Type.\n",
    "# crosswalk = crosswalk.drop_duplicates(subset=['Cell Type'])\n",
    "\n",
    "# # For each row in D265, replace Cell Type with the corresponding CL Label from crosswalk file. In case of no match, keep the original Cell Type. \n",
    "# # In case the Cell Type matches but the CL Label field in the crosswalk file is empty, keep the original Cell Type.\n",
    "# df1 = pd.merge(df1, crosswalk, on='Cell Type', how='left')\n",
    "# df1['Cell Type'] = np.where(df1['CL Label'].isnull(), df1['Cell Type'], df1['CL Label'])\n",
    "# df1 = df1.drop(columns=['CL Label'])\n",
    "\n",
    "# Drop Cell Ontology ID and Cell Ontology ID_2 columns.\n",
    "df1 = df1.drop(columns=['Cell Ontology ID', 'Cell Ontology ID_2'])\n",
    "\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "label = \"D265-LLL-7A7-12\"\n",
    "# Save the updated D265 csv file.\n",
    "df1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "id = f\"https://entity.api.hubmapconsortium.org/entities/0f1ddcb41a484adbda759c0c79097a02#{label}\"\n",
    "\n",
    "dataset_json = {\n",
    "    \"@id\": id\n",
    "}\n",
    "\n",
    "with open(f\"{target_dir}/{label}-dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset_json, f, indent=4)\n",
    "\n",
    "del df1\n",
    "# Replace cell type names with cell labels from crosswalk file and generate cell-nodes files for both datasets. \n",
    "\n",
    "# Read D115 csv file.\n",
    "df1 = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"D115_final_annotated_cells.csv\"))\n",
    "\n",
    "# Drop duplicate rows in D115. Check if any duplicate rows. If true, print them.\n",
    "df1 = df1.drop_duplicates(subset=['x', 'y', 'Cell Type'])\n",
    "\n",
    "# # Read crosswalk file.\n",
    "# crosswalk = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"extras\", \"combined_unique_ct_list_with_cl_labels.csv\"))\n",
    "# # Keep unique rows in crosswalk file. Check uniqueness based on Cell Type.\n",
    "# crosswalk = crosswalk.drop_duplicates(subset=['Cell Type'])\n",
    "\n",
    "# # For each row in D115, replace Cell Type with the corresponding CL Label from crosswalk file. In case of no match, keep the original Cell Type. \n",
    "# # In case the Cell Type matches but the CL Label field in the crosswalk file is empty, keep the original Cell Type.\n",
    "# df1 = pd.merge(df1, crosswalk, on='Cell Type', how='left')\n",
    "# df1['Cell Type'] = np.where(df1['CL Label'].isnull(), df1['Cell Type'], df1['CL Label'])\n",
    "# df1 = df1.drop(columns=['CL Label'])\n",
    "\n",
    "# Drop Cell Ontology ID and Cell Ontology ID_2 columns.\n",
    "df1 = df1.drop(columns=['Cell Ontology ID', 'Cell Ontology ID_2'])\n",
    "\n",
    "label = \"D115-RLL-10A3-40\"\n",
    "# Save the updated D115 csv file.\n",
    "df1.to_csv(f\"{target_dir}/{label}-nodes.csv\", index=False, header=True, mode=\"w\")\n",
    "\n",
    "id = f\"https://entity.api.hubmapconsortium.org/entities/0f1ddcb41a484adbda759c0c79097a02#{label}\"\n",
    "\n",
    "dataset_json = {\n",
    "    \"@id\": id\n",
    "}\n",
    "\n",
    "with open(f\"{target_dir}/{label}-dataset.json\", \"w\") as f:\n",
    "    json.dump(dataset_json, f, indent=4)\n",
    "\n",
    "# Print the unique cell types in both datasets combined.\n",
    "unique_cell_types = pd.concat([df1['Cell Type'], df1['Cell Type']]).unique()\n",
    "print(unique_cell_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## bonemarrow-codex-chop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Erythroid' 'B-Cells' 'AEC' 'Early Myeloid Progenitor' 'SEC'\n",
      " 'Intermediate Myeloid' 'Mature Myeloid' 'CD8+ T-Cell' 'Plasma Cells'\n",
      " 'Erythroblast' 'Adipocyte' 'Monocytes' 'Adipo-MSC' 'Endosteal'\n",
      " 'THY1+ MSC' 'CD4+ T-Cell' 'GMP/Myeloblast' 'GATA1pos_Mks'\n",
      " 'Immature_B_Cell' 'Macrophages' 'SPINK2+ HSPC' 'pDC'\n",
      " 'Non-Classical Monocyte' 'GATA1neg_Mks' 'HSPC' 'VSMC' 'GMP'\n",
      " 'MEP/Early Erythroblast' 'CLP' 'CD34+ CD61+' 'HSC' 'NPM1 Mutant Blast'\n",
      " 'Schwann Cells' 'Artifact' 'Undetermined' 'Autofluorescent'\n",
      " 'CD44+ Undetermined']\n",
      "Directory '/u/yashjain/hra-cell-distance-analysis/data/data-processed-nodes/bonemarrow-codex-chop' created successfully.\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"bonemarrow-codex-chop\"\n",
    "\n",
    "metadata1 = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"AML_NSM_RefMap_Seurat_v2_seurat_metadata.csv\"))\n",
    "\n",
    "metadata2 = pd.read_csv(os.path.join(basepath, orig_filedir, dataset_name, \"Normal_Bone_Marrow_CODEX_Atlas_Seurat_v2_seurat_metadata.csv\"))\n",
    "\n",
    "# Extract x, y coords, cell type, and filename from metadata1 into a new dataframe.\n",
    "metadata1['x'] = metadata1['x.coord']\n",
    "metadata1['y'] = metadata1['y.coord']\n",
    "metadata1['Cell Type'] = metadata1['classified_cluster_anno_l2']\n",
    "metadata1['filename'] = metadata1['orig.ident']\n",
    "metadata1 = metadata1[['x', 'y', 'Cell Type', 'filename']]\n",
    "\n",
    "# Extract x, y coords, cell type, and filename from metadata2 into a new dataframe.\n",
    "metadata2['x'] = metadata2['x.coord']\n",
    "metadata2['y'] = metadata2['y.coord']\n",
    "metadata2['Cell Type'] = metadata2['cluster_anno_l2']\n",
    "metadata2['filename'] = metadata2['orig.ident']\n",
    "metadata2 = metadata2[['x', 'y', 'Cell Type', 'filename']]\n",
    "\n",
    "# Merge metadata1 and metadata2.\n",
    "data_merged = pd.concat([metadata1, metadata2], axis=0)\n",
    "data_merged = data_merged.reset_index(drop=True)\n",
    "\n",
    "# Remove \"_CODEX_Mesmer\" from filename.\n",
    "data_merged['filename'] = data_merged['filename'].str.replace('_CODEX_Mesmer', '')\n",
    "\n",
    "# Write data_merged to a csv file.\n",
    "data_merged.to_csv(os.path.join(basepath, orig_filedir, dataset_name, \"data_merged.csv\"), index=False)\n",
    "\n",
    "# Print unique cell types in the data_merged dataframe.\n",
    "print(data_merged['Cell Type'].unique())\n",
    "\n",
    "# Iterate over unique values in data_merged column filename and write each subset to a csv file in vccf-data-cell-nodes/published/bonemarrow-codex-chop directory. Drop filename column.\n",
    "target_dir = os.path.join(basepath, dest_filedir, dataset_name)\n",
    "create_directory(target_dir)\n",
    "\n",
    "for filename in data_merged['filename'].unique():\n",
    "    data_subset = data_merged[data_merged['filename'] == filename]\n",
    "    data_subset = data_subset.drop(columns=['filename'])\n",
    "    data_subset.to_csv(f'{target_dir}/{filename}-nodes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All datasets processed and saved.\n"
     ]
    }
   ],
   "source": [
    "# Print final message\n",
    "print(\"All datasets processed and saved.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hra-cell-distance-analysis-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
